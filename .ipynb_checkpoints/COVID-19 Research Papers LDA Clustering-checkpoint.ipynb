{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Research Papers LDA Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jayfeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jayfeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jayfeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Reads in abstracts.csv and filters out rows with missing values.\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"abstracts.csv\")\n",
    "df = df[df[\"abstract\"] != \"NaN\"]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Set up stop words, stemmer, and lemmatizer.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "snowBallStemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tokenize and clean the abstracts of every paper.\"\"\"\n",
    "\n",
    "def tokenize_clean(abstract):\n",
    "    #tokenizes abstract string\n",
    "    tokens = word_tokenize(abstract.lower())\n",
    "    \n",
    "    #lemmatizes tokens\n",
    "    counter = 0\n",
    "    while counter < len(tokens):\n",
    "        tokens[counter] = lemmatizer.lemmatize(tokens[counter])\n",
    "        counter += 1\n",
    "    \n",
    "    #filters, stems, and lowercases tokens\n",
    "    filtered_tokens = []\n",
    "    for i in tokens:\n",
    "        if i not in stop_words and len(i) > 3 and i != \"abstract\":\n",
    "            stemmed_word = snowBallStemmer.stem(i)\n",
    "            filtered_tokens.append(stemmed_word)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "df[\"abstract tokens\"] = df.apply(lambda row: tokenize_clean(row.abstract), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Perform LDA topic modelling on a sample of the papers for speed purposes.\"\"\"\n",
    "\n",
    "partial_df = df.sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a list of lists of cleaned tokens of abstracts\"\"\"\n",
    "\n",
    "partial_texts = []\n",
    "for index, row in partial_df.iterrows():\n",
    "    partial_texts.append(row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Use gensim package to perform LDA topic modelling.\n",
    "Code from https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "used for reference.\n",
    "\"\"\"\n",
    "\n",
    "dictionary = corpora.Dictionary(partial_texts)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in partial_texts]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_results = ldamodel.show_topics(num_topics=10, num_words=15, formatted=False)\n",
    "#lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['health',\n",
       "  'outbreak',\n",
       "  'public',\n",
       "  'care',\n",
       "  'diseas',\n",
       "  'case',\n",
       "  'countri',\n",
       "  'china',\n",
       "  'emerg',\n",
       "  'risk',\n",
       "  'studi',\n",
       "  'sar',\n",
       "  'control',\n",
       "  'report',\n",
       "  'infect'],\n",
       " ['patient',\n",
       "  'respiratori',\n",
       "  'infect',\n",
       "  'virus',\n",
       "  'influenza',\n",
       "  'child',\n",
       "  'case',\n",
       "  'studi',\n",
       "  'clinic',\n",
       "  'sever',\n",
       "  'viral',\n",
       "  'pneumonia',\n",
       "  'hospit',\n",
       "  'detect',\n",
       "  'acut'],\n",
       " ['protein',\n",
       "  'structur',\n",
       "  'virus',\n",
       "  'domain',\n",
       "  'bind',\n",
       "  'activ',\n",
       "  'viral',\n",
       "  'membran',\n",
       "  'interact',\n",
       "  'cell',\n",
       "  'function',\n",
       "  'studi',\n",
       "  'acid',\n",
       "  'target',\n",
       "  'coronavirus'],\n",
       " ['vaccin',\n",
       "  'virus',\n",
       "  'antibodi',\n",
       "  'immun',\n",
       "  'respons',\n",
       "  'influenza',\n",
       "  'infect',\n",
       "  'antigen',\n",
       "  'protect',\n",
       "  'neutral',\n",
       "  'human',\n",
       "  'epitop',\n",
       "  'develop',\n",
       "  'challeng',\n",
       "  'high'],\n",
       " ['diseas',\n",
       "  'cancer',\n",
       "  'regul',\n",
       "  'inflamm',\n",
       "  'inflammatori',\n",
       "  'process',\n",
       "  'chronic',\n",
       "  'system',\n",
       "  'factor',\n",
       "  'model',\n",
       "  'viral',\n",
       "  'mechan',\n",
       "  'role',\n",
       "  'infect',\n",
       "  'review'],\n",
       " ['diseas',\n",
       "  'develop',\n",
       "  'review',\n",
       "  'potenti',\n",
       "  'research',\n",
       "  'effect',\n",
       "  'includ',\n",
       "  'human',\n",
       "  'drug',\n",
       "  'also',\n",
       "  'approach',\n",
       "  'pathogen',\n",
       "  'activ',\n",
       "  'provid',\n",
       "  'current'],\n",
       " ['detect',\n",
       "  'virus',\n",
       "  'sampl',\n",
       "  'use',\n",
       "  'assay',\n",
       "  'strain',\n",
       "  'test',\n",
       "  'gene',\n",
       "  'method',\n",
       "  'sensit',\n",
       "  'result',\n",
       "  'specif',\n",
       "  'studi',\n",
       "  'posit',\n",
       "  'pedv'],\n",
       " ['virus',\n",
       "  'model',\n",
       "  'sequenc',\n",
       "  'transmiss',\n",
       "  'use',\n",
       "  'data',\n",
       "  'genom',\n",
       "  'human',\n",
       "  'popul',\n",
       "  'studi',\n",
       "  'differ',\n",
       "  'analysi',\n",
       "  'speci',\n",
       "  'estim',\n",
       "  'diseas'],\n",
       " ['diseas',\n",
       "  'group',\n",
       "  'patient',\n",
       "  'calf',\n",
       "  'treatment',\n",
       "  'clinic',\n",
       "  'infect',\n",
       "  'studi',\n",
       "  'effect',\n",
       "  'signific',\n",
       "  'increas',\n",
       "  'anim',\n",
       "  'caus',\n",
       "  'associ',\n",
       "  'sever'],\n",
       " ['cell',\n",
       "  'infect',\n",
       "  'virus',\n",
       "  'express',\n",
       "  'activ',\n",
       "  'viral',\n",
       "  'respons',\n",
       "  'mous',\n",
       "  'immun',\n",
       "  'replic',\n",
       "  'induc',\n",
       "  'host',\n",
       "  'protein',\n",
       "  'studi',\n",
       "  'result']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create topic_words, a list of buckets of words that represent each topic.\"\"\"\n",
    "\n",
    "topic_words = []\n",
    "#i is each topic\n",
    "for i in lda_results:\n",
    "    topic_i_words = []\n",
    "    word_pairs = i[1]\n",
    "    #j is the list of word-probability pairs (we don't care about the probabilities here)\n",
    "    for j in word_pairs:\n",
    "        topic_i_words.append(j[0])\n",
    "    topic_words.append(topic_i_words)\n",
    "        \n",
    "topic_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sha</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract tokens</th>\n",
       "      <th>assigned topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15651</td>\n",
       "      <td>41231</td>\n",
       "      <td>d9e4c9b6b809ddc1f9dc8787f77368334e1e538b</td>\n",
       "      <td>Abstract Introduction Sources describing the g...</td>\n",
       "      <td>[introduct, sourc, describ, global, burden, em...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2170</td>\n",
       "      <td>29e0c4a7f3f8e2bd4a8f50fae12cc31f1a863763</td>\n",
       "      <td>Surveillance is the ongoing, systematic collec...</td>\n",
       "      <td>[surveil, ongo, systemat, collect, analysi, in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14806</td>\n",
       "      <td>28624</td>\n",
       "      <td>eaca17432584c7f2ecbb17e611df70deed0dbec3</td>\n",
       "      <td>Since the World Health Organization declared t...</td>\n",
       "      <td>[sinc, world, health, organ, declar, global, o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12915</td>\n",
       "      <td>16518</td>\n",
       "      <td>ccdc714272fd3392147edfbc7bf0731811c2b674</td>\n",
       "      <td>Summary This study describes a loophole in the...</td>\n",
       "      <td>[summari, studi, describ, loophol, intern, qua...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25734</td>\n",
       "      <td>40820</td>\n",
       "      <td>5794890e355a8abbec51b46d767b6ee67edcd274</td>\n",
       "      <td>The first human Zika virus (ZIKV) outbreak was...</td>\n",
       "      <td>[first, human, zika, virus, zikv, outbreak, re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15610</td>\n",
       "      <td>41114</td>\n",
       "      <td>f6525bc180629aca4b0760e0ff9f187f2e2a6fec</td>\n",
       "      <td>Abstract Swine acute diarrhea syndrome coronav...</td>\n",
       "      <td>[swine, acut, diarrhea, syndrom, coronavirus, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16232</td>\n",
       "      <td>42779</td>\n",
       "      <td>616525aa865b829ccba0cc30c39be871bbbe7a18</td>\n",
       "      <td>Abstract Respiratory syncytial virus (RSV) is ...</td>\n",
       "      <td>[respiratori, syncyti, virus, lead, caus, lowe...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21285</td>\n",
       "      <td>22207</td>\n",
       "      <td>cbe56b09d64047cba4ee7875c4f55276a0cdf273</td>\n",
       "      <td>BACKGROUND: We previously reported that Entero...</td>\n",
       "      <td>[background, previous, report, enterovirus, ev...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22953</td>\n",
       "      <td>23901</td>\n",
       "      <td>d18636f47e3c7dd93da309d556ba464d964fd24f</td>\n",
       "      <td>Hantavirus infection, which causes zoonotic di...</td>\n",
       "      <td>[hantavirus, infect, caus, zoonot, diseas, hig...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8983</td>\n",
       "      <td>11504</td>\n",
       "      <td>e257181519299dcc054236cf425e6c85ef5e7cb4</td>\n",
       "      <td>Abstract P53, a vital anticancer gene, control...</td>\n",
       "      <td>[vital, anticanc, gene, control, transcript, t...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                       sha  \\\n",
       "15651       41231  d9e4c9b6b809ddc1f9dc8787f77368334e1e538b   \n",
       "1390         2170  29e0c4a7f3f8e2bd4a8f50fae12cc31f1a863763   \n",
       "14806       28624  eaca17432584c7f2ecbb17e611df70deed0dbec3   \n",
       "12915       16518  ccdc714272fd3392147edfbc7bf0731811c2b674   \n",
       "25734       40820  5794890e355a8abbec51b46d767b6ee67edcd274   \n",
       "...           ...                                       ...   \n",
       "15610       41114  f6525bc180629aca4b0760e0ff9f187f2e2a6fec   \n",
       "16232       42779  616525aa865b829ccba0cc30c39be871bbbe7a18   \n",
       "21285       22207  cbe56b09d64047cba4ee7875c4f55276a0cdf273   \n",
       "22953       23901  d18636f47e3c7dd93da309d556ba464d964fd24f   \n",
       "8983        11504  e257181519299dcc054236cf425e6c85ef5e7cb4   \n",
       "\n",
       "                                                abstract  \\\n",
       "15651  Abstract Introduction Sources describing the g...   \n",
       "1390   Surveillance is the ongoing, systematic collec...   \n",
       "14806  Since the World Health Organization declared t...   \n",
       "12915  Summary This study describes a loophole in the...   \n",
       "25734  The first human Zika virus (ZIKV) outbreak was...   \n",
       "...                                                  ...   \n",
       "15610  Abstract Swine acute diarrhea syndrome coronav...   \n",
       "16232  Abstract Respiratory syncytial virus (RSV) is ...   \n",
       "21285  BACKGROUND: We previously reported that Entero...   \n",
       "22953  Hantavirus infection, which causes zoonotic di...   \n",
       "8983   Abstract P53, a vital anticancer gene, control...   \n",
       "\n",
       "                                         abstract tokens  assigned topic  \n",
       "15651  [introduct, sourc, describ, global, burden, em...               0  \n",
       "1390   [surveil, ongo, systemat, collect, analysi, in...               0  \n",
       "14806  [sinc, world, health, organ, declar, global, o...               0  \n",
       "12915  [summari, studi, describ, loophol, intern, qua...               0  \n",
       "25734  [first, human, zika, virus, zikv, outbreak, re...               0  \n",
       "...                                                  ...             ...  \n",
       "15610  [swine, acut, diarrhea, syndrom, coronavirus, ...               9  \n",
       "16232  [respiratori, syncyti, virus, lead, caus, lowe...               9  \n",
       "21285  [background, previous, report, enterovirus, ev...               9  \n",
       "22953  [hantavirus, infect, caus, zoonot, diseas, hig...               9  \n",
       "8983   [vital, anticanc, gene, control, transcript, t...               9  \n",
       "\n",
       "[3000 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Assign a topic to each of the papers.\"\"\"\n",
    "\n",
    "assigned_topic = []\n",
    "for index, row in partial_df.iterrows():\n",
    "    tokens = row[\"abstract tokens\"]\n",
    "    counter_array = [0] * 15\n",
    "    for i in tokens:\n",
    "        for j in np.arange(10):\n",
    "            if i in topic_words[j]:\n",
    "                counter_array[j] += 1\n",
    "    max_topic = counter_array.index(max(counter_array))\n",
    "    assigned_topic.append(max_topic)\n",
    "                \n",
    "partial_df[\"assigned topic\"] = assigned_topic\n",
    "partial_df = partial_df.sort_values(\"assigned topic\")\n",
    "partial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Research Paper Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read in the metadata csv, select only the papers with full texts available.\n",
    "- Split the df into four dfs according to the license type, so its easier to find the corresponding full text file\n",
    "- Drops rows where the \"sha\" corresponds to more than one full text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Split up metadata.csv into separate dfs based on licence type.\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"CORD-19-research-challenge/metadata.csv\")\n",
    "df_with_full = df.loc[df['has_full_text'] == True].drop(columns = 'has_full_text')\n",
    "\n",
    "df_custom_license = df_with_full.loc[df_with_full['full_text_file'].str.match(\"custom_license\")]\n",
    "df_noncomm_use_subset = df_with_full.loc[df_with_full['full_text_file'].str.match(\"noncomm_use_subset\")]\n",
    "df_comm_use_subset = df_with_full.loc[df_with_full['full_text_file'].str.match(\"comm_use_subset\")]\n",
    "df_biorxiv_medrxiv = df_with_full.loc[df_with_full['full_text_file'].str.match(\"biorxiv_medrxiv\")]\n",
    "\n",
    "invalid_rows = []\n",
    "for index, row in df_custom_license.iterrows():\n",
    "    if \";\" in row['sha']:\n",
    "        invalid_rows.append(index)\n",
    "\n",
    "df_custom_license = df_custom_license.drop(invalid_rows)\n",
    "\n",
    "invalid_rows = []\n",
    "for index, row in df_noncomm_use_subset.iterrows():\n",
    "    if \";\" in row['sha']:\n",
    "        invalid_rows.append(index)\n",
    "        \n",
    "df_noncomm_use_subset = df_noncomm_use_subset.drop(invalid_rows)\n",
    "\n",
    "invalid_rows = []\n",
    "for index, row in df_comm_use_subset.iterrows():\n",
    "    if \";\" in row['sha']:\n",
    "        invalid_rows.append(index)\n",
    "        \n",
    "df_comm_use_subset = df_comm_use_subset.drop(invalid_rows)\n",
    "\n",
    "invalid_rows = []\n",
    "for index, row in df_biorxiv_medrxiv.iterrows():\n",
    "    if \";\" in row['sha']:\n",
    "        invalid_rows.append(index)\n",
    "        \n",
    "df_biorxiv_medrxiv = df_biorxiv_medrxiv.drop(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Method takes in the full text file of paper and returns all the text as a string.\"\"\"\n",
    "\n",
    "def json_to_body_string(body_json):\n",
    "    body_text = body_json[0][3]\n",
    "    body_text_json = json.dumps(body_text)\n",
    "    body_text_df = pd.read_json(body_text_json, orient=\"records\")\n",
    "    body_string = \"\"\n",
    "    for i in np.arange(len(body_text_df)):\n",
    "        if (isinstance(body_text_df[\"text\"][i], str)):\n",
    "            body_string += body_text_df[\"text\"][i]\n",
    "        else:\n",
    "            not_strings.append(type(body_text_df[\"text\"][i]))\n",
    "    return body_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Method takes in df_license (one of the four license dfs) and license_type (its license type as a string),\n",
    "and adds the contents of each paper into full_texts_dict.\n",
    "\"\"\"\n",
    "\n",
    "def compile_full_texts(df_license, license_type):\n",
    "    for i in df_license[\"sha\"]:\n",
    "        temp_json = pd.read_json(\"CORD-19-research-challenge/\" + license_type +\n",
    "                                 \"/\" + license_type + \"/\" + i + \".json\", orient=\"index\")\n",
    "        full_text_string = json_to_body_string(temp_json)\n",
    "        full_texts_dict[i] = full_text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_texts_dict = {}\n",
    "# not_strings = []\n",
    "# for i in df_custom_license[\"sha\"]:\n",
    "#     temp_json = pd.read_json(\"CORD-19-research-challenge/custom_license/custom_license/\" + i + \".json\", orient=\"index\")\n",
    "#     full_text_string = json_to_body_string(temp_json)\n",
    "#     full_texts_dict[i] = full_text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compiles the full texts into the dictionary full_texts_dict, which contains each paper \"sha\" mapped to its\n",
    "contents as a string.\n",
    "\"\"\"\n",
    "\n",
    "full_texts_dict = {}\n",
    "not_strings = []\n",
    "compile_full_texts(df_custom_license, \"custom_license\")\n",
    "compile_full_texts(df_noncomm_use_subset, \"noncomm_use_subset\")\n",
    "compile_full_texts(df_comm_use_subset, \"comm_use_subset\")\n",
    "compile_full_texts(df_biorxiv_medrxiv, \"biorxiv_medrxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_texts_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

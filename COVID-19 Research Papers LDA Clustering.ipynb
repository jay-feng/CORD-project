{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Research Papers LDA Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jayfeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jayfeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jayfeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Reads in abstracts.csv and filters out rows with missing values.\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"abstracts.csv\")\n",
    "df = df[df[\"abstract\"] != \"NaN\"]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_func(row):\n",
    "#     return word_tokenize(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df[\"abstract tokens\"] = df.apply(lambda row: tokenize_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# def lemmatize_func(row):\n",
    "#     counter = 0\n",
    "#     while counter < len(row[3]):\n",
    "#         row[3][counter] = lemmatizer.lemmatize(row[3][counter])\n",
    "#         counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"abstract tokens\"] = df.apply(lambda row: tokenize_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "snowBallStemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_abstract = 'Abstract Middle-aged female identical twins, one of whom had systemic lupus erythematosus (SLE), were evaluated for immunologic reactivity to previous antigenic challenges, including primary immunization with a foreign antigen, keyhole limpet hemocyanin (KLH). These two women had lived together for all of their 58 years and neither was receiving anti-inflammatory or immunosuppressive drugs at the time of these studies. Both twins demonstrated comparable 7S and 198 humoral antibody response to KLH, as well as similar viral antibody titers. However, the twin with SLE was anergic to common antigens, streptokinase-streptodornase, Trichophyton and Candida; furthermore delayed hypersensitivity to KLH did not develop after immunization. This observed discrepancy between humoral and cellular immunity in genetically similar subjects may be significant in the pathogenesis of SLE.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_clean(abstract):\n",
    "    #tokenizes abstract string\n",
    "    tokens = word_tokenize(abstract.lower())\n",
    "    \n",
    "    #lemmatizes tokens\n",
    "    counter = 0\n",
    "    while counter < len(tokens):\n",
    "        tokens[counter] = lemmatizer.lemmatize(tokens[counter])\n",
    "        counter += 1\n",
    "    \n",
    "    #filters, stems, and lowercases tokens\n",
    "    filtered_tokens = []\n",
    "    for i in tokens:\n",
    "        if i not in stop_words and len(i) > 3 and i != \"abstract\":\n",
    "            stemmed_word = snowBallStemmer.stem(i)\n",
    "            filtered_tokens.append(stemmed_word)\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"abstract tokens\"] = df.apply(lambda row: tokenize_clean(row.abstract), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sha</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>aecbc613ebdab36753235197ffb4f35734b5ca63</td>\n",
       "      <td>Abstract Middle-aged female identical twins, o...</td>\n",
       "      <td>[middle-ag, femal, ident, twin, system, lupus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>212e990b378e8d267042753d5f9d4a64ea5e9869</td>\n",
       "      <td>Abstract Our understanding of the pathogenesis...</td>\n",
       "      <td>[understand, pathogenesi, infecti, especi, bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>bf5d344243153d58be692ceb26f52c08e2bd2d2f</td>\n",
       "      <td>Abstract In the pathogenesis of rheumatoid art...</td>\n",
       "      <td>[pathogenesi, rheumatoid, arthriti, local, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>ddd2ecf42ec86ad66072962081e1ce4594431f9c</td>\n",
       "      <td>Abstract Pharyngitis, bronchitis, and pneumoni...</td>\n",
       "      <td>[pharyng, bronchiti, pneumonia, repres, common...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>a55cb4e724091ced46b5e55b982a14525eea1c7e</td>\n",
       "      <td>Abstract Acute bronchitis, an illness frequent...</td>\n",
       "      <td>[acut, bronchiti, ill, frequent, encount, prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27685</td>\n",
       "      <td>40508</td>\n",
       "      <td>179df1e769292dd113cef1b54b0b43213e6b5c97</td>\n",
       "      <td>Background/introduction COVID−19, a novel coro...</td>\n",
       "      <td>[background/introduct, covid−19, novel, corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27686</td>\n",
       "      <td>40509</td>\n",
       "      <td>9b4445849937393a4b05378653521a9d0c34dc8e</td>\n",
       "      <td>Governments around the world must rapidly mobi...</td>\n",
       "      <td>[govern, around, world, must, rapid, mobil, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27687</td>\n",
       "      <td>40510</td>\n",
       "      <td>4e618ec5d2edea031a9ff8058a9bafafe30937be</td>\n",
       "      <td>The 2019-Novel-Coronavirus (COVID-19) has affe...</td>\n",
       "      <td>[2019-novel-coronavirus, covid-19, affect, cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27688</td>\n",
       "      <td>40511</td>\n",
       "      <td>28b53e0cab53b10ab87431d6cc4ac1e0a7c4d6b9</td>\n",
       "      <td>Object Meteorological parameters are the impor...</td>\n",
       "      <td>[object, meteorolog, paramet, import, factor, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27689</td>\n",
       "      <td>40513</td>\n",
       "      <td>210b30ec755f5d8b0622c78a8e570290293bf49f</td>\n",
       "      <td>We report temporal patterns of viral shedding ...</td>\n",
       "      <td>[report, tempor, pattern, viral, shed, laborat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24045 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                       sha  \\\n",
       "0               3  aecbc613ebdab36753235197ffb4f35734b5ca63   \n",
       "1               5  212e990b378e8d267042753d5f9d4a64ea5e9869   \n",
       "2               6  bf5d344243153d58be692ceb26f52c08e2bd2d2f   \n",
       "3               7  ddd2ecf42ec86ad66072962081e1ce4594431f9c   \n",
       "4               8  a55cb4e724091ced46b5e55b982a14525eea1c7e   \n",
       "...           ...                                       ...   \n",
       "27685       40508  179df1e769292dd113cef1b54b0b43213e6b5c97   \n",
       "27686       40509  9b4445849937393a4b05378653521a9d0c34dc8e   \n",
       "27687       40510  4e618ec5d2edea031a9ff8058a9bafafe30937be   \n",
       "27688       40511  28b53e0cab53b10ab87431d6cc4ac1e0a7c4d6b9   \n",
       "27689       40513  210b30ec755f5d8b0622c78a8e570290293bf49f   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      Abstract Middle-aged female identical twins, o...   \n",
       "1      Abstract Our understanding of the pathogenesis...   \n",
       "2      Abstract In the pathogenesis of rheumatoid art...   \n",
       "3      Abstract Pharyngitis, bronchitis, and pneumoni...   \n",
       "4      Abstract Acute bronchitis, an illness frequent...   \n",
       "...                                                  ...   \n",
       "27685  Background/introduction COVID−19, a novel coro...   \n",
       "27686  Governments around the world must rapidly mobi...   \n",
       "27687  The 2019-Novel-Coronavirus (COVID-19) has affe...   \n",
       "27688  Object Meteorological parameters are the impor...   \n",
       "27689  We report temporal patterns of viral shedding ...   \n",
       "\n",
       "                                         abstract tokens  \n",
       "0      [middle-ag, femal, ident, twin, system, lupus,...  \n",
       "1      [understand, pathogenesi, infecti, especi, bac...  \n",
       "2      [pathogenesi, rheumatoid, arthriti, local, pro...  \n",
       "3      [pharyng, bronchiti, pneumonia, repres, common...  \n",
       "4      [acut, bronchiti, ill, frequent, encount, prim...  \n",
       "...                                                  ...  \n",
       "27685  [background/introduct, covid−19, novel, corona...  \n",
       "27686  [govern, around, world, must, rapid, mobil, ma...  \n",
       "27687  [2019-novel-coronavirus, covid-19, affect, cou...  \n",
       "27688  [object, meteorolog, paramet, import, factor, ...  \n",
       "27689  [report, tempor, pattern, viral, shed, laborat...  \n",
       "\n",
       "[24045 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# texts = []\n",
    "# for index, row in df.iterrows():\n",
    "#     texts.append(row[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(ldamodel.print_topics(num_topics=20, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(0, '0.044*\"hcov\" + 0.029*\"measl\" + 0.026*\"facial\" + 0.021*\"hcov-nl63\" + 0.018*\"rhiniti\"'),\n",
    " (1, '0.080*\"virus\" + 0.047*\"infect\" + 0.034*\"respiratori\" + 0.030*\"viral\" + 0.020*\"detect\"'),\n",
    " (2, '0.052*\"cell\" + 0.031*\"infect\" + 0.021*\"respons\" + 0.019*\"express\" + 0.019*\"immun\"'),\n",
    " (3, '0.165*\"januari\" + 0.094*\"decemb\" + 0.022*\"affili\" + 0.013*\"aviat\" + 0.011*\"thailand\"'),\n",
    " (4, '0.075*\"patient\" + 0.026*\"sever\" + 0.026*\"clinic\" + 0.020*\"diseas\" + 0.015*\"treatment\"'),\n",
    " (5, '0.040*\"activ\" + 0.037*\"antivir\" + 0.028*\"effect\" + 0.026*\"drug\" + 0.021*\"inhibit\"'),\n",
    " (6, '0.051*\"model\" + 0.017*\"number\" + 0.017*\"epidem\" + 0.013*\"popul\" + 0.013*\"estim\"'),\n",
    " (7, '0.042*\"sequenc\" + 0.030*\"gene\" + 0.028*\"strain\" + 0.027*\"genom\" + 0.018*\"analysi\"'),\n",
    " (8, '0.022*\"diseas\" + 0.014*\"review\" + 0.013*\"develop\" + 0.011*\"pathogen\" + 0.011*\"system\"'),\n",
    " (9, '0.035*\"use\" + 0.033*\"detect\" + 0.028*\"assay\" + 0.025*\"method\" + 0.024*\"test\"'),\n",
    " (10, '0.093*\"pedv\" + 0.073*\"porcin\" + 0.046*\"swine\" + 0.043*\"diarrhea\" + 0.043*\"piglet\"'),\n",
    " (11, '0.028*\"health\" + 0.024*\"outbreak\" + 0.016*\"public\" + 0.014*\"china\" + 0.014*\"diseas\"'),\n",
    " (12, '0.088*\"korea\" + 0.051*\"felin\" + 0.019*\"lesion\" + 0.015*\"spectrometri\" + 0.014*\"periton\"'),\n",
    " (13, '0.090*\"vaccin\" + 0.056*\"antibodi\" + 0.036*\"immun\" + 0.021*\"protect\" + 0.019*\"respons\"'),\n",
    " (14, '0.039*\"case\" + 0.033*\"hospit\" + 0.028*\"patient\" + 0.020*\"rate\" + 0.013*\"infect\"'),\n",
    " (15, '0.034*\"studi\" + 0.029*\"use\" + 0.020*\"data\" + 0.019*\"method\" + 0.018*\"result\"'),\n",
    " (16, '0.041*\"protein\" + 0.024*\"virus\" + 0.015*\"cell\" + 0.015*\"viral\" + 0.011*\"structur\"'),\n",
    " (17, '0.087*\"temperatur\" + 0.060*\"heat\" + 0.044*\"inactiv\" + 0.028*\"skin\" + 0.027*\"humid\"'),\n",
    " (18, '0.044*\"group\" + 0.025*\"blood\" + 0.016*\"calf\" + 0.016*\"signific\" + 0.015*\"level\"'),\n",
    " (19, '0.072*\"influenza\" + 0.022*\"pandem\" + 0.021*\"particip\" + 0.019*\"health\" + 0.016*\"work\"')]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df = df#.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_texts = []\n",
    "for index, row in partial_df.iterrows():\n",
    "    partial_texts.append(row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(partial_texts)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in partial_texts]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(ldamodel.print_topics(num_topics=15, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('detect', 0.033920556),\n",
       "   ('sampl', 0.027166747),\n",
       "   ('assay', 0.019891832),\n",
       "   ('use', 0.01736933),\n",
       "   ('virus', 0.016513366),\n",
       "   ('test', 0.01598823),\n",
       "   ('sensit', 0.011024779),\n",
       "   ('pedv', 0.010967567),\n",
       "   ('result', 0.010411655),\n",
       "   ('infect', 0.010170441),\n",
       "   ('method', 0.009833486),\n",
       "   ('specif', 0.0096131945),\n",
       "   ('calf', 0.009167769),\n",
       "   ('serum', 0.009100203),\n",
       "   ('antibodi', 0.0090053165)]),\n",
       " (1,\n",
       "  [('virus', 0.053980086),\n",
       "   ('infect', 0.05109838),\n",
       "   ('cell', 0.043939605),\n",
       "   ('viral', 0.035461344),\n",
       "   ('replic', 0.022380788),\n",
       "   ('antivir', 0.017703585),\n",
       "   ('activ', 0.014894153),\n",
       "   ('inhibit', 0.012344222),\n",
       "   ('host', 0.011745705),\n",
       "   ('effect', 0.008913329),\n",
       "   ('express', 0.008135327),\n",
       "   ('induc', 0.006751869),\n",
       "   ('result', 0.006548479),\n",
       "   ('entri', 0.0062152296),\n",
       "   ('studi', 0.0061823665)]),\n",
       " (2,\n",
       "  [('protein', 0.047007475),\n",
       "   ('structur', 0.013329896),\n",
       "   ('bind', 0.011568938),\n",
       "   ('use', 0.008860905),\n",
       "   ('sars-cov', 0.008660932),\n",
       "   ('domain', 0.008599563),\n",
       "   ('express', 0.00849282),\n",
       "   ('antibodi', 0.008425309),\n",
       "   ('membran', 0.008305879),\n",
       "   ('peptid', 0.007190046),\n",
       "   ('coronavirus', 0.0071627093),\n",
       "   ('cell', 0.0070531573),\n",
       "   ('acid', 0.006593161),\n",
       "   ('interact', 0.006070015),\n",
       "   ('function', 0.0060508433)]),\n",
       " (3,\n",
       "  [('effect', 0.020097999),\n",
       "   ('activ', 0.016410738),\n",
       "   ('use', 0.014272691),\n",
       "   ('studi', 0.012778553),\n",
       "   ('result', 0.0124175055),\n",
       "   ('level', 0.011154294),\n",
       "   ('temperatur', 0.010770873),\n",
       "   ('increas', 0.010433093),\n",
       "   ('signific', 0.009738305),\n",
       "   ('concentr', 0.009293216),\n",
       "   ('compound', 0.008825636),\n",
       "   ('group', 0.007959889),\n",
       "   ('decreas', 0.0078864675),\n",
       "   ('treatment', 0.007743156),\n",
       "   ('show', 0.007052256)]),\n",
       " (4,\n",
       "  [('vaccin', 0.031448007),\n",
       "   ('infect', 0.020393202),\n",
       "   ('immun', 0.020196743),\n",
       "   ('respons', 0.015831403),\n",
       "   ('mous', 0.014653637),\n",
       "   ('diseas', 0.012346584),\n",
       "   ('lung', 0.010919255),\n",
       "   ('protect', 0.010270102),\n",
       "   ('antibodi', 0.0082929395),\n",
       "   ('level', 0.008187829),\n",
       "   ('develop', 0.0076830704),\n",
       "   ('sever', 0.007483311),\n",
       "   ('cell', 0.006730786),\n",
       "   ('caus', 0.006243764),\n",
       "   ('clinic', 0.00618007)]),\n",
       " (5,\n",
       "  [('cell', 0.031006906),\n",
       "   ('express', 0.01645803),\n",
       "   ('immun', 0.016235724),\n",
       "   ('respons', 0.014207809),\n",
       "   ('gene', 0.0127573265),\n",
       "   ('role', 0.011195028),\n",
       "   ('function', 0.011173234),\n",
       "   ('mechan', 0.009183238),\n",
       "   ('activ', 0.0087411),\n",
       "   ('diseas', 0.008270201),\n",
       "   ('pathway', 0.007693741),\n",
       "   ('regul', 0.0072261905),\n",
       "   ('signal', 0.006902428),\n",
       "   ('host', 0.0066330316),\n",
       "   ('system', 0.0065493193)]),\n",
       " (6,\n",
       "  [('patient', 0.05115433),\n",
       "   ('respiratori', 0.023444727),\n",
       "   ('infect', 0.017119778),\n",
       "   ('hospit', 0.014678408),\n",
       "   ('studi', 0.013752409),\n",
       "   ('clinic', 0.013578048),\n",
       "   ('sever', 0.013332246),\n",
       "   ('child', 0.012070971),\n",
       "   ('virus', 0.01123585),\n",
       "   ('influenza', 0.010934871),\n",
       "   ('group', 0.009856331),\n",
       "   ('associ', 0.009147836),\n",
       "   ('symptom', 0.008820579),\n",
       "   ('case', 0.008256766),\n",
       "   ('pneumonia', 0.0080842925)]),\n",
       " (7,\n",
       "  [('health', 0.015227364),\n",
       "   ('diseas', 0.014384168),\n",
       "   ('develop', 0.010883988),\n",
       "   ('use', 0.009333263),\n",
       "   ('system', 0.007733148),\n",
       "   ('public', 0.0070640366),\n",
       "   ('research', 0.00682269),\n",
       "   ('method', 0.0066466373),\n",
       "   ('data', 0.006446825),\n",
       "   ('emerg', 0.0063187694),\n",
       "   ('provid', 0.0062949765),\n",
       "   ('review', 0.0061093825),\n",
       "   ('studi', 0.0060607456),\n",
       "   ('approach', 0.00552681),\n",
       "   ('infecti', 0.005322742)]),\n",
       " (8,\n",
       "  [('virus', 0.045950424),\n",
       "   ('sequenc', 0.02109854),\n",
       "   ('human', 0.018895034),\n",
       "   ('genom', 0.015737327),\n",
       "   ('strain', 0.015694315),\n",
       "   ('gene', 0.011716973),\n",
       "   ('influenza', 0.010452128),\n",
       "   ('viral', 0.010020694),\n",
       "   ('speci', 0.00979775),\n",
       "   ('analysi', 0.009337433),\n",
       "   ('pathogen', 0.0086512),\n",
       "   ('genet', 0.007970704),\n",
       "   ('isol', 0.0077164453),\n",
       "   ('host', 0.0073199216),\n",
       "   ('studi', 0.0072354414)]),\n",
       " (9,\n",
       "  [('covid-19', 0.030508587),\n",
       "   ('case', 0.019190846),\n",
       "   ('outbreak', 0.014833069),\n",
       "   ('model', 0.012647507),\n",
       "   ('epidem', 0.0119463615),\n",
       "   ('china', 0.011790718),\n",
       "   ('transmiss', 0.009190143),\n",
       "   ('number', 0.009082511),\n",
       "   ('infect', 0.009080673),\n",
       "   ('wuhan', 0.008958926),\n",
       "   ('spread', 0.008709969),\n",
       "   ('rate', 0.0083019035),\n",
       "   ('sars-cov-2', 0.008091144),\n",
       "   ('health', 0.0076877535),\n",
       "   ('measur', 0.0076357643)])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results = ldamodel.show_topics(num_topics=10, num_words=15, formatted=False)\n",
    "word_pairs = lda_results[2][1]\n",
    "word_pairs[2][0]\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "#i is each topic\n",
    "for i in lda_results:\n",
    "    topic_i_words = []\n",
    "    word_pairs = i[1]\n",
    "    #j is the list of word-probability pairs (we don't care about the probabilities here)\n",
    "    for j in word_pairs:\n",
    "        topic_i_words.append(j[0])\n",
    "    topic_words.append(topic_i_words)\n",
    "        \n",
    "#topic_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0] * 10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [middle-ag, femal, ident, twin, system, lupus,...\n",
       "1        [understand, pathogenesi, infecti, especi, bac...\n",
       "2        [pathogenesi, rheumatoid, arthriti, local, pro...\n",
       "3        [pharyng, bronchiti, pneumonia, repres, common...\n",
       "4        [acut, bronchiti, ill, frequent, encount, prim...\n",
       "                               ...                        \n",
       "27685    [background/introduct, covid−19, novel, corona...\n",
       "27686    [govern, around, world, must, rapid, mobil, ma...\n",
       "27687    [2019-novel-coronavirus, covid-19, affect, cou...\n",
       "27688    [object, meteorolog, paramet, import, factor, ...\n",
       "27689    [report, tempor, pattern, viral, shed, laborat...\n",
       "Name: abstract tokens, Length: 24045, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_df[\"abstract tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_df\n",
    "\n",
    "assigned_topic = []\n",
    "for index, row in partial_df.iterrows():\n",
    "    tokens = row[\"abstract tokens\"]\n",
    "    counter_array = [0] * 15\n",
    "    for i in tokens:\n",
    "        for j in np.arange(10):\n",
    "            if i in topic_words[j]:\n",
    "                counter_array[j] += 1\n",
    "    max_topic = counter_array.index(max(counter_array))\n",
    "    assigned_topic.append(max_topic)\n",
    "                \n",
    "assigned_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24045"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(assigned_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df[\"assigned topic\"] = assigned_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract This study describes the isolation and characterization of an influenza virus subtype H3N2 designated A/Swine/Weybridge/163266/87. The virus was isolated from a severe outbreak of respiratory disease in East Anglia. Haemagglutinin and neuraminidase characterization showed the virus to be very similar to H3N2 strains circulating in the human population during the years 1972–1975, and to H3N2 strains recently isolated from pigs in Belgium and France. A serological survey showed antibodies to the virus to be present in 31% of pigs tested, and reactors were detected on 43% of farms sampled.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_df[\"abstract\"][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_df[\"assigned topic\"][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
